{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a53ba7-b3e8-4cfc-aec8-89e3457ab505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NGC_API_KEY\"] = \"nvapi-****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed7057-c14c-4cde-81b3-be8f9794791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a387f2-0ee0-4fff-99fa-1837f7a71ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, subprocess, time\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Setup NeMo/NIM cache\n",
    "# -------------------------------\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = \"/ephemeral/cache/nim\"\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)\n",
    "print(f\"LOCAL_NIM_CACHE set to {os.environ['LOCAL_NIM_CACHE']}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Setup Docker ephemeral storage\n",
    "# -------------------------------\n",
    "storage_path = \"/ephemeral/cache/docker\"\n",
    "os.makedirs(storage_path, exist_ok=True)\n",
    "\n",
    "daemon_file = \"/etc/docker/daemon.json\"\n",
    "config = {}\n",
    "try:\n",
    "    config = json.load(open(daemon_file)) if os.path.exists(daemon_file) else {}\n",
    "except PermissionError:\n",
    "    print(\"Cannot read daemon.json. Run with sudo or check path.\")\n",
    "\n",
    "# Update Docker root\n",
    "config[\"data-root\"] = storage_path\n",
    "config_str = json.dumps(config, indent=4)\n",
    "\n",
    "# Write daemon.json (requires sudo)\n",
    "subprocess.run(f\"echo '{config_str}' | sudo tee {daemon_file} > /dev/null\", shell=True, check=True)\n",
    "\n",
    "# Restart Docker\n",
    "subprocess.run(\"sudo systemctl restart docker\", shell=True, check=True)\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify new Docker root\n",
    "docker_root = subprocess.run(\n",
    "    \"docker info | grep 'Docker Root Dir'\",\n",
    "    shell=True, capture_output=True, text=True\n",
    ").stdout.strip()\n",
    "print(\"Docker Root Dir:\", docker_root)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Setup pip cache\n",
    "# -------------------------------\n",
    "pip_cache = \"/ephemeral/cache/pip\"\n",
    "os.makedirs(pip_cache, exist_ok=True)\n",
    "os.environ[\"PIP_CACHE_DIR\"] = pip_cache\n",
    "print(f\"PIP_CACHE_DIR set to {pip_cache}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Setup HuggingFace cache\n",
    "# -------------------------------\n",
    "hf_cache = \"/ephemeral/cache/huggingface\"\n",
    "os.makedirs(hf_cache, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "print(f\"HF_HOME set to {hf_cache}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Setup tmpdir\n",
    "# -------------------------------\n",
    "tmp_dir = \"/ephemeral/tmp\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "os.environ[\"TMPDIR\"] = tmp_dir\n",
    "print(f\"TMPDIR set to {tmp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320ff51-186f-47d9-914d-f5b41e7986b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus all --name nemo-rl -it \\\n",
    "  -p 9000:9000 \\\n",
    "  -v \"$(pwd)\":/workspace \\\n",
    "  -w /workspace \\\n",
    "  -d nvcr.io/nvidia/nemo-rl:v0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298c159-540b-4603-b3d0-ec5112300f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "\n",
    "!docker exec {container} bash -c \"git clone https://github.com/NVIDIA-NeMo/RL.git nemo-rl --recursive\"\n",
    "!docker exec {container} bash -c \"cd nemo-rl && git submodule update --init --recursive\"\n",
    "\n",
    "# Activate NeMo RL venv\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate\"\n",
    "\n",
    "# HuggingFace login\n",
    "!docker exec {container} bash -c \"huggingface-cli login --token hf_********\"\n",
    "\n",
    "# WANDB API key\n",
    "!docker exec {container} bash -c 'export WANDB_API_KEY=\"*****\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca800a68-7d8c-4a4d-a42f-41c71a5154ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "\n",
    "!docker exec -it $container bash -c 'source /opt/nemo_rl_venv/bin/activate && \\\n",
    "uv run python nemo-rl/examples/run_dpo.py \\\n",
    "cluster.gpus_per_node=1 \\\n",
    "dpo.max_num_steps=10 \\\n",
    "policy.model_name=meta-llama/Llama-3.2-1B-Instruct \\\n",
    "policy.tokenizer.name=meta-llama/Llama-3.2-1B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb74402-c499-4f3f-b720-a84212b4bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate && \\\n",
    "    uv run nemo-rl/examples/converters/convert_dcp_to_hf.py \\\n",
    "    --config ./results/dpo/step_10/config.yaml \\\n",
    "    --dcp-ckpt-path ./results/dpo/step_10/policy/weights \\\n",
    "    --hf-ckpt-path ./results/dpo/step_10/hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f4852-5d99-4ad7-9c4d-98079ad899df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "hf_path = \"./results/dpo/step_10/hf/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_path, torch_dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"How does photosynthesis work in plants?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc920a-1e0f-4930-8ddd-32e6ceff6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate && python inference.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654332a-d760-4924-860c-3f6dd25a726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile convert.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "src = \"./results/dpo/step_10/hf\"\n",
    "dst = \"./results/dpo/step_10/hf_st\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(src)\n",
    "model.save_pretrained(dst, safe_serialization=True)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(src)\n",
    "tok.save_pretrained(dst)\n",
    "\n",
    "print(\"Saved to:\", dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dc77f-d47b-4b2d-b700-ca4af89c1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"nemo-rl\"\n",
    "!docker exec {container} bash -c \"source /opt/nemo_rl_venv/bin/activate && python convert.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49709a84-1f6a-40fc-ad6a-3ca570fe0b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "#   MultiLLM-NIM Container Launcher\n",
    "#   (Detached mode)\n",
    "# ===============================\n",
    "\n",
    "# Choose container name\n",
    "CONTAINER_NAME = \"MultiLLM-NIM\"\n",
    "\n",
    "# NGC Multi-LLM NIM repo\n",
    "Repository = \"nim/nvidia/llm-nim\"\n",
    "TAG = \"latest\"\n",
    "IMG_NAME = f\"nvcr.io/{Repository}:{TAG}\"\n",
    "\n",
    "# Path to your local HF DPO model\n",
    "LOCAL_MODEL_DIR = \"./results/dpo/step_10/hf_st\"\n",
    "\n",
    "# Name to expose the served model\n",
    "NIM_SERVED_MODEL_NAME = \"dpo-llm\"\n",
    "\n",
    "# Local NIM cache (you chose ephemeral)\n",
    "LOCAL_NIM_CACHE = \"/ephemeral/cache/nim\"\n",
    "\n",
    "# Create cache directory\n",
    "!mkdir -p \"{LOCAL_NIM_CACHE}\"\n",
    "!chmod -R a+w \"{LOCAL_NIM_CACHE}\"\n",
    "\n",
    "print(\"Starting MultiLLM-NIM container in detached mode...\")\n",
    "print(\"Container:\", CONTAINER_NAME)\n",
    "print(\"Image:\", IMG_NAME)\n",
    "print(\"Model Path:\", LOCAL_MODEL_DIR)\n",
    "print(\"NIM Cache:\", LOCAL_NIM_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451aa29-82a8-4f19-9873-aaedd122cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Run the container DETACHED\n",
    "# -------------------------------\n",
    "!docker run -d --rm --name={CONTAINER_NAME} \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_PROFILE=\"e2f00b2cbfb168f907c8d6d4d40406f7261111fbab8b3417a485dcd19d10cc98\" \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/local_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME={NIM_SERVED_MODEL_NAME} \\\n",
    "  -v \"{LOCAL_MODEL_DIR}:/opt/models/local_model\" \\\n",
    "  -v \"{LOCAL_NIM_CACHE}:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  {IMG_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb0b9c-39ff-4709-889f-e2e891f79b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/v1/health/ready' #make sure the LLM NIM port is correct\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "print(\"Checking MultiLLM NIM readiness...\")\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get(\"message\") == \"Service is ready.\":\n",
    "                print(\"LLM NIM is ready.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"LLM NIM is not ready. Waiting for 30 seconds...\")\n",
    "        else:\n",
    "            print(f\"Unexpected status code {response.status_code}. Waiting for 30 seconds...\")\n",
    "    except requests.ConnectionError:\n",
    "        print(\"LLM NIM is not ready. Waiting for 30 seconds...\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1d8d9-4056-4c1c-afbc-0ec3ee2485e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST 'http://localhost:8000/v1/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"model\": \"dpo-llm\", \"prompt\": \"The sky appears blue because\", \"max_tokens\": 64}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33416929-22c2-44e1-9b1c-6e418364ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop {CONTAINER_NAME}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
