{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing Your Own LangGraph Agent to NeMo Agent Toolkit\n",
    "\n",
    "In this notebook, we'll show you how to integrate an existing LangGraph agent with the NeMo Agent Toolkit (NAT).\n",
    "\n",
    "You'll learn how to wrap LangGraph agents so they work smoothly with NAT. This lets you take advantage of NAT features like MCP compatibility, observability, optimization, and profiling in your existing LangGraph agent systems without refactoring your existing code.\n",
    "\n",
    "**Key Difference**: Unlike traditional LangChain agents that use the `AgentExecutor` pattern, LangGraph uses a **graph-based architecture** with nodes and edges, providing more flexibility and control over agent execution flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [0.0) Setup](#setup)\n",
    "  - [0.1) Prerequisites](#prereqs)\n",
    "  - [0.2) API Keys](#api-keys)\n",
    "  - [0.3) Installing NeMo Agent Toolkit](#installing-nat)\n",
    "- [1.0) Defining an 'Existing' LangGraph Agent](#defining-existing-agent)\n",
    "- [2.0) Existing Agent Migration](#migration)\n",
    "  - [2.1) Migration Part 1: Transforming Your Existing Agent into a Workflow](#migration-part-1)\n",
    "  - [2.2) Migration Part 2: Making Your Agent Configurable](#migration-part-2)\n",
    "  - [2.3) Migration Part 3: Integration with NeMo Agent Toolkit](#migration-part-3)\n",
    "  - [2.4) Migration Part 4: A Zero-Code Configuration](#migration-part-4)\n",
    "- [3) Next Steps](#next-steps)\n",
    "\n",
    "<span style=\"color:rgb(0, 31, 153); font-style: italic;\">Note: In Google Colab use the Table of Contents tab to navigate.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# 0.0) Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prereqs\"></a>\n",
    "## 0.1) Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Platform:** Linux, macOS, or Windows\n",
    "- **Python:** version 3.11, 3.12, or 3.13\n",
    "- **Python Packages:** `pip`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"api-keys\"></a>\n",
    "## 0.2) API Keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will need the following API keys to run all examples end-to-end:\n",
    "\n",
    "- **NVIDIA Build:** You can obtain an NVIDIA Build API Key by creating an [NVIDIA Build](https://build.nvidia.com) account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "- **Tavily:** You can obtain a Tavily API Key by creating a [Tavily](https://www.tavily.com/) account and generating a key at https://app.tavily.com/home\n",
    "\n",
    "Then you can run the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# For local NIM deployment, you may not need an NVIDIA API key\n",
    "# If your local NIM requires authentication, set it here\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    # For local NIM, you can use a placeholder or skip this\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key (or press Enter for local NIM): \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key or \"not-needed-for-local-nim\"\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    tavily_api_key = getpass.getpass(\"Enter your Tavily API key: \")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Using Local NIM Deployment\n",
    "\n",
    "This notebook has been configured to use a **local NVIDIA NIM** deployment instead of the NVIDIA Build API.\n",
    "\n",
    "**Your Local NIM Configuration:**\n",
    "- **Endpoint**: `http://0.0.0.0:8000/v1`\n",
    "- **Model**: `meta-llama/llama-3.1-8b-instruct`\n",
    "\n",
    "**To verify your local NIM is running:**\n",
    "```bash\n",
    "curl -X 'POST' \\\n",
    "  'http://0.0.0.0:8000/v1/chat/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"meta-llama/llama-3.1-8b-instruct\",\n",
    "    \"messages\": [{\"role\":\"user\", \"content\":\"Hello!\"}],\n",
    "    \"max_tokens\": 64\n",
    "  }'\n",
    "```\n",
    "\n",
    "**Benefits of Local NIM:**\n",
    "- ‚úÖ No API key required (or use local auth)\n",
    "- ‚úÖ Lower latency (no internet roundtrip)\n",
    "- ‚úÖ Better privacy (data stays local)\n",
    "- ‚úÖ No rate limits\n",
    "- ‚úÖ Cost savings (no per-token charges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Changes for Local NIM\n",
    "\n",
    "The following configurations have been updated throughout this notebook to use your local NIM:\n",
    "\n",
    "**1. Config Files Updated:**\n",
    "- `langgraph_agent_workflow/configs/config.yml`\n",
    "- `langgraph_agent_workflow/configs/config_8b.yml`\n",
    "- `langgraph_agent_workflow/configs/config_with_profiling.yml`\n",
    "\n",
    "**2. Key Changes Made:**\n",
    "\n",
    "```yaml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta-llama/llama-3.1-8b-instruct  # Changed from meta/llama-3.3-70b-instruct\n",
    "    base_url: http://0.0.0.0:8000/v1              # Added: Points to your local NIM\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "```\n",
    "\n",
    "**3. For Direct Code Usage (Python scripts):**\n",
    "\n",
    "If you're creating standalone scripts, use:\n",
    "\n",
    "```python\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "    base_url=\"http://0.0.0.0:8000/v1\",  # Your local NIM endpoint\n",
    "    temperature=0.2,\n",
    "    max_tokens=2048,\n",
    "    api_key=\"not-needed-for-local-nim\"  # Use placeholder or actual key if required\n",
    ")\n",
    "```\n",
    "\n",
    "**4. Testing Your Local NIM:**\n",
    "\n",
    "Run this cell to verify connectivity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your local NIM connection\n",
    "import requests\n",
    "import json\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://0.0.0.0:8000/v1/chat/completions',\n",
    "        headers={\n",
    "            'accept': 'application/json',\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        json={\n",
    "            \"model\": \"meta-llama/llama-3.1-8b-instruct\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Say 'Hello from local NIM!' in one sentence.\"}],\n",
    "            \"max_tokens\": 64\n",
    "        },\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ Local NIM is running!\")\n",
    "        print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: Status {response.status_code}\")\n",
    "        print(response.text)\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Cannot connect to local NIM at http://0.0.0.0:8000\")\n",
    "    print(\"Please ensure your NIM is running.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Summary of Changes for Local NIM Usage\n",
    "\n",
    "**What Changed:**\n",
    "\n",
    "| Component | Original (NVIDIA Build) | Updated (Local NIM) |\n",
    "|-----------|------------------------|---------------------|\n",
    "| **Endpoint** | `https://integrate.api.nvidia.com/v1` | `http://0.0.0.0:8000/v1` |\n",
    "| **Model Name** | `meta/llama-3.3-70b-instruct` | `meta-llama/llama-3.1-8b-instruct` |\n",
    "| **API Key** | Required from NVIDIA Build | Optional/Placeholder |\n",
    "| **Network** | Internet required | Local only |\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "1. **Model Availability**: Ensure your local NIM has the `meta-llama/llama-3.1-8b-instruct` model loaded\n",
    "2. **Multiple Models**: If you have other models in your NIM, you can change `model_name` to any available model\n",
    "3. **Authentication**: If your local NIM requires authentication, set the `api_key` parameter appropriately\n",
    "4. **Network**: Replace `0.0.0.0` with `localhost` or specific IP if needed\n",
    "\n",
    "**Checking Available Models:**\n",
    "\n",
    "```bash\n",
    "curl http://0.0.0.0:8000/v1/models\n",
    "```\n",
    "\n",
    "This will list all models available in your local NIM deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"installing-nat\"></a>\n",
    "## 0.3) Installing NeMo Agent Toolkit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way to install NAT is through `pip` or `uv pip`.\n",
    "\n",
    "First, we will install `uv` which offers parallel downloads and faster dependency resolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies\n",
    "\n",
    "**Note**: If you encounter `No module named pip` errors, the cells below will automatically fix this by:\n",
    "1. Ensuring pip is installed in your environment\n",
    "2. Using the correct Python interpreter from your kernel\n",
    "\n",
    "**Alternative methods** if you continue to have issues:\n",
    "- Option 1: Run in terminal: `python3 -m ensurepip --default-pip`\n",
    "- Option 2: Reinstall the virtual environment\n",
    "- Option 3: Use system Python instead of venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check your Python environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Virtual env: {os.getenv('VIRTUAL_ENV', 'Not in a virtual environment')}\")\n",
    "\n",
    "# Try to import pip\n",
    "try:\n",
    "    import pip\n",
    "    print(f\"‚úÖ pip is installed (version {pip.__version__})\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå pip is not installed - will be fixed in next cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /home/ubuntu/.venv/lib/python3.12/site-packages (0.9.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m ensurepip --default-pip\n",
    "!{sys.executable} -m pip install uv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: If the above doesn't work, run this in your terminal:\n",
    "# cd to your project directory, then run:\n",
    "# python3 -m venv .venv --system-site-packages\n",
    "# source .venv/bin/activate  # On macOS/Linux\n",
    "# .venv\\Scripts\\activate     # On Windows\n",
    "# python -m ensurepip --upgrade\n",
    "# pip install jupyter ipykernel\n",
    "# python -m ipykernel install --user --name=.venv\n",
    "\n",
    "print(\"If you're still having issues, run the commands above in your terminal,\")\n",
    "print(\"then restart the Jupyter kernel and select the .venv kernel.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeMo Agent toolkit can be installed through the PyPI `nvidia-nat` package.\n",
    "\n",
    "There are several optional subpackages available for NAT. The `langchain` subpackage contains useful components for integrating and running with [LangChain](https://python.langchain.com/docs/introduction/) and [LangGraph](https://langchain-ai.github.io/langgraph/).\n",
    "\n",
    "**Note**: LangGraph is part of the LangChain ecosystem and is included when you install `nvidia-nat[langchain]`. This single installation provides both LangChain and LangGraph dependencies.\n",
    "\n",
    "Since LangGraph will be used later in this notebook, let's install NAT with the optional `langchain` subpackage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-nat[langchain] is already installed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "uv pip show -q \"nvidia-nat-langchain\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    uv pip install \"nvidia-nat[langchain]\"\n",
    "else\n",
    "    echo \"nvidia-nat[langchain] is already installed\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that both LangChain and LangGraph are available:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"defining-existing-agent\"></a>\n",
    "# 1.0) Defining an 'Existing' LangGraph Agent\n",
    "\n",
    "In this case study, we will use a simple, self-contained LangGraph agent as a proxy for your 'existing' agent. This agent comes equipped with a search tool that is capable of retrieving context from the internet using the Tavily API.\n",
    "\n",
    "**Key Difference from LangChain**: Unlike traditional LangChain agents that use the `AgentExecutor` pattern, LangGraph uses a **graph-based architecture** with nodes and edges. This provides more flexibility and control over the agent's execution flow.\n",
    "\n",
    "The cell below defines a simple LangGraph agent with a string input query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def existing_agent_main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python langgraph_agent.py \\\"Your question here\\\"\")\n",
    "        sys.exit(1)\n",
    "    user_input = sys.argv[1]\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=5,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Initialize a LLM client (using local NIM)\n",
    "    llm = ChatNVIDIA(\n",
    "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "        base_url=\"http://0.0.0.0:8000/v1\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=2048,\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\", \"not-needed-for-local-nim\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent using the prebuilt function\n",
    "    # This creates a StateGraph with agent and tool nodes automatically\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # Invoke the agent with a user query\n",
    "    # LangGraph uses message-based state\n",
    "    response = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
    "\n",
    "    # Extract and print the final response\n",
    "    final_message = response[\"messages\"][-1]\n",
    "    print(final_message.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    existing_agent_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main components to this LangGraph agent:\n",
    "\n",
    "* **a web search tool (Tavily)** - for retrieving information from the internet\n",
    "\n",
    "* **an LLM (Llama 3.1 8B via local NIM)** - for reasoning and generating responses\n",
    "\n",
    "* **a graph-based agent system (LangGraph's `create_react_agent`)** - for orchestrating the agent's execution\n",
    "\n",
    "The agent is constructed using LangGraph's `create_react_agent` function, which automatically creates a state graph with:\n",
    "- An **agent node** that calls the LLM\n",
    "- **Tool nodes** for executing tools\n",
    "- **Conditional edges** for routing between agent and tools\n",
    "\n",
    "We pass the requested input into the graph and get a response back through the message state.\n",
    "\n",
    "All of the components in use come from LangGraph/LangChain, but any other framework or example could also work.\n",
    "\n",
    "Next we will run this sample agent to validate that it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on LangGraph vs LangChain Parameters**: \n",
    "\n",
    "We're using the same parameters as the LangChain example (`max_results=2`, `temperature=0.0`, `max_completion_tokens=1024`) for consistency. However, you may notice that LangGraph's `create_react_agent` sometimes produces different quality responses compared to LangChain's `AgentExecutor` with identical settings.\n",
    "\n",
    "This is because:\n",
    "- **Different default system prompts** between the frameworks\n",
    "- **Different agent execution patterns** (graph-based vs. executor-based)\n",
    "- **Different tool result handling** in the reasoning loop\n",
    "\n",
    "If you see incomplete responses like *\"was not specified in the search results\"*, you can improve this by:\n",
    "```python\n",
    "# Increase search results for more context\n",
    "search = TavilySearch(max_results=5, api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "# Slightly higher temperature and more tokens (using local NIM)\n",
    "llm = ChatNVIDIA(\n",
    "    model=\"meta-llama/llama-3.1-8b-instruct\", \n",
    "    base_url=\"http://0.0.0.0:8000/v1\",\n",
    "    temperature=0.2, \n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "tools = [search]\n",
    "graph = create_react_agent(model=llm, tools=tools)\n",
    "```\n",
    "\n",
    "Let's test the basic version first to see how it performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks/langgraph_agent.py:21: DeprecationWarning: The 'max_tokens' parameter is deprecated and will be removed in a future version. Please use 'max_completion_tokens' instead.\n",
      "  llm = ChatNVIDIA(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current World Cup holder is the Argentina national team, who defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n"
     ]
    }
   ],
   "source": [
    "!python langgraph_agent.py \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration\"></a>\n",
    "# 2.0) Existing Agent Migration\n",
    "\n",
    "<a id=\"migration-part-1\"></a>\n",
    "## 2.1) Migration Part 1: Transforming Your Existing Agent into a Workflow\n",
    "\n",
    "NAT supports users bringing their own agent into the framework. As the primary entrypoint for agent execution is a NAT Workflow. For the first pass at NAT migration we will create a new workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow 'langgraph_agent_workflow' already exists.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat workflow create langgraph_agent_workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a workflow directory for a new agent, we will continue by migrating the agent's functional code into the new workflow. In the next cell, we have adapted the agent code from the `def existing_agent_main()` into a new method `def langgraph_agent_workflow_function()` which encapsulates the exact same functionality, but is decorated and registered for NAT workflow compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n",
    "import logging\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LangGraphAgentWorkflowFunctionConfig(FunctionBaseConfig, name=\"langgraph_agent_workflow\"):\n",
    "    pass\n",
    "\n",
    "\n",
    "@register_function(config_type=LangGraphAgentWorkflowFunctionConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def langgraph_agent_workflow_function(_config: LangGraphAgentWorkflowFunctionConfig, _builder: Builder):\n",
    "    import os\n",
    "\n",
    "    from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "    from langchain_tavily import TavilySearch\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=5,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Initialize a LLM client (using local NIM)\n",
    "    llm = ChatNVIDIA(\n",
    "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "        base_url=\"http://0.0.0.0:8000/v1\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=2048,\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\", \"not-needed-for-local-nim\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent using the prebuilt function\n",
    "    # This creates a StateGraph with agent and tool nodes automatically\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    async def _response_fn(input_message: str) -> str:\n",
    "        response = graph.invoke({\"messages\": [(\"user\", input_message)]})\n",
    "        final_message = response[\"messages\"][-1]\n",
    "        return final_message.content\n",
    "\n",
    "    yield FunctionInfo.from_fn(_response_fn, description=\"A simple LangGraph agent capable of basic internet search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, this is almost the exact same code as your 'existing' LangGraph agent, but has been refactored to fit within a NAT function registration.\n",
    "\n",
    "The only differences are:\n",
    "1. The definition of a closure function `_response_fn` which captures the instantiated graph and uses that to invoke the agent and return the response\n",
    "2. The use of the `@register_function` decorator\n",
    "3. The async function signature for NAT compatibility\n",
    "\n",
    "**Key Difference from LangChain Migration**:\n",
    "- LangChain agents use `agent_executor.invoke({\"input\": ..., \"chat_history\": []})` and return `response[\"output\"]`\n",
    "- LangGraph agents use `graph.invoke({\"messages\": [(\"user\", ...)]})` and return `response[\"messages\"][-1].content`\n",
    "\n",
    "We can also simplify the workflow configuration to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config.yml\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  \n",
    "# Note: Using local NIM deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can run the new workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:01 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config.yml'\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 0\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:16 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team, who defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration-part-2\"></a>\n",
    "## 2.2) Migration Part 2: Making Your Agent Configurable\n",
    "\n",
    "Now that we have a working NAT workflow, let's make it more configurable. We'll parameterize the model name, temperature, and other settings so they can be controlled through the YAML configuration file.\n",
    "\n",
    "This makes the agent more flexible and easier to experiment with different configurations without changing the code.\n",
    "\n",
    "**Improving Response Quality**: If you noticed vague responses in the previous run, we'll address this by:\n",
    "- Increasing `max_search_results` from 2 to 5 (more context)\n",
    "- Raising `temperature` from 0.0 to 0.2 (less conservative reasoning)\n",
    "- Increasing `max_tokens` from 1024 to 2048 (fuller responses)\n",
    "- Enabling `verbose` mode to see the agent's reasoning process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n",
    "import logging\n",
    "\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LangGraphAgentWorkflowFunctionConfig(FunctionBaseConfig, name=\"langgraph_agent_workflow\"):\n",
    "    \"\"\"Configuration for the LangGraph agent workflow.\"\"\"\n",
    "    model_name: str = Field(\n",
    "        default=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "        description=\"The name of the LLM model to use\"\n",
    "    )\n",
    "    base_url: str = Field(\n",
    "        default=\"http://0.0.0.0:8000/v1\",\n",
    "        description=\"Base URL for the local NIM endpoint\"\n",
    "    )\n",
    "    temperature: float = Field(\n",
    "        default=0.2,\n",
    "        description=\"Temperature for LLM sampling\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "    max_tokens: int = Field(\n",
    "        default=2048,\n",
    "        description=\"Maximum number of completion tokens in the response\",\n",
    "        gt=0\n",
    "    )\n",
    "    max_search_results: int = Field(\n",
    "        default=5,\n",
    "        description=\"Maximum number of search results to retrieve\",\n",
    "        gt=0,\n",
    "        le=10\n",
    "    )\n",
    "    verbose: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Enable verbose logging\"\n",
    "    )\n",
    "\n",
    "\n",
    "@register_function(config_type=LangGraphAgentWorkflowFunctionConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def langgraph_agent_workflow_function(config: LangGraphAgentWorkflowFunctionConfig, _builder: Builder):\n",
    "    import os\n",
    "\n",
    "    from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "    from langchain_tavily import TavilySearch\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    if config.verbose:\n",
    "        logger.info(f\"Initializing LangGraph agent with model: {config.model_name}\")\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=config.max_search_results,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Initialize a LLM client with configurable parameters (using local NIM)\n",
    "    llm = ChatNVIDIA(\n",
    "        model=config.model_name,\n",
    "        base_url=config.base_url,\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\", \"not-needed-for-local-nim\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    async def _response_fn(input_message: str) -> str:\n",
    "        \"\"\"Execute the LangGraph agent and return the response.\"\"\"\n",
    "        if config.verbose:\n",
    "            logger.info(f\"Processing input: {input_message}\")\n",
    "        \n",
    "        response = graph.invoke({\"messages\": [(\"user\", input_message)]})\n",
    "        final_message = response[\"messages\"][-1]\n",
    "        \n",
    "        if config.verbose:\n",
    "            logger.info(f\"Generated response: {final_message.content}\")\n",
    "        \n",
    "        return final_message.content\n",
    "\n",
    "    yield FunctionInfo.from_fn(_response_fn, description=\"A configurable LangGraph agent capable of internet search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a more detailed configuration file that takes advantage of these parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config.yml\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  model_name: meta/llama-3.3-70b-instruct\n",
    "  temperature: 0.2\n",
    "  max_completion_tokens: 2048\n",
    "  max_search_results: 5\n",
    "  verbose: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reinstall the workflow for the changes to take effect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinstalling workflow 'langgraph_agent_workflow'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow 'langgraph_agent_workflow' reinstalled successfully.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat workflow reinstall langgraph_agent_workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the updated configurable workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:22 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config.yml'\n",
      "2025-12-04 11:05:22 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:53 - Initializing LangGraph agent with model: meta/llama-3.3-70b-instruct\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 0\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:22 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:81 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:05:24 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:87 - Generated response: The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:05:24 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration-part-3\"></a>\n",
    "## 2.3) Migration Part 3: Integration with NeMo Agent Toolkit\n",
    "\n",
    "Now let's take it a step further and integrate the LangGraph agent with other NAT components. We can use NAT's built-in LLM management and make the agent use NAT-managed LLMs instead of directly instantiating them.\n",
    "\n",
    "This provides several benefits:\n",
    "- Better observability and tracing\n",
    "- Consistent LLM usage across workflows\n",
    "- Easy model switching through configuration\n",
    "- Integration with NAT's profiling and optimization tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n",
    "import logging\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.component_ref import LLMRef\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LangGraphAgentWorkflowFunctionConfig(FunctionBaseConfig, name=\"langgraph_agent_workflow\"):\n",
    "    \"\"\"Configuration for the LangGraph agent workflow integrated with NAT.\"\"\"\n",
    "    llm_name: LLMRef = Field(\n",
    "        description=\"Reference to the NAT-managed LLM to use for the agent\"\n",
    "    )\n",
    "    max_search_results: int = Field(\n",
    "        default=2,\n",
    "        description=\"Maximum number of search results to retrieve\",\n",
    "        gt=0,\n",
    "        le=10\n",
    "    )\n",
    "    verbose: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Enable verbose logging\"\n",
    "    )\n",
    "\n",
    "\n",
    "@register_function(config_type=LangGraphAgentWorkflowFunctionConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def langgraph_agent_workflow_function(config: LangGraphAgentWorkflowFunctionConfig, builder: Builder):\n",
    "    import os\n",
    "\n",
    "    from langchain_tavily import TavilySearch\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    if config.verbose:\n",
    "        logger.info(f\"Initializing LangGraph agent with NAT-managed LLM: {config.llm_name}\")\n",
    "\n",
    "    # Get the LLM from NAT's builder with LangChain wrapper\n",
    "    llm = await builder.get_llm(config.llm_name, wrapper_type=LLMFrameworkEnum.LANGCHAIN)\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=config.max_search_results,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent using NAT-managed LLM\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    async def _response_fn(input_message: str) -> str:\n",
    "        \"\"\"Execute the LangGraph agent and return the response.\"\"\"\n",
    "        if config.verbose:\n",
    "            logger.info(f\"Processing input: {input_message}\")\n",
    "        \n",
    "        response = graph.invoke({\"messages\": [(\"user\", input_message)]})\n",
    "        final_message = response[\"messages\"][-1]\n",
    "        \n",
    "        if config.verbose:\n",
    "            logger.info(f\"Generated response: {final_message.content}\")\n",
    "        \n",
    "        return final_message.content\n",
    "\n",
    "    yield FunctionInfo.from_fn(\n",
    "        _response_fn, \n",
    "        description=\"A NAT-integrated LangGraph agent capable of internet search using Tavily\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**‚öôÔ∏è Local NIM Quick Check:**\n",
    "\n",
    "Before running the workflow, ensure your local NIM is accessible:\n",
    "\n",
    "```bash\n",
    "# Quick health check\n",
    "curl http://0.0.0.0:8000/v1/models\n",
    "\n",
    "# Test inference\n",
    "curl -X POST http://0.0.0.0:8000/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"model\": \"meta-llama/llama-3.1-8b-instruct\", \"messages\": [{\"role\":\"user\",\"content\":\"Hi\"}], \"max_tokens\": 10}'\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now update the configuration to use NAT's LLM management:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta-llama/llama-3.1-8b-instruct\n",
    "    base_url: http://0.0.0.0:8000/v1\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinstall and test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinstalling workflow 'langgraph_agent_workflow'...\n",
      "Workflow 'langgraph_agent_workflow' reinstalled successfully.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat workflow reinstall langgraph_agent_workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:30 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config.yml'\n",
      "2025-12-04 11:05:30 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:40 - Initializing LangGraph agent with NAT-managed LLM: nim_llm\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 1\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:30 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:05:32 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The winner of the last World Cup was Argentina. They defeated France in the 2022 World Cup final with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:05:32 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The winner of the last World Cup was Argentina. They defeated France in the 2022 World Cup final with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration-part-4\"></a>\n",
    "## 2.4) Migration Part 4: A Zero-Code Configuration\n",
    "\n",
    "Now that we have a fully integrated LangGraph agent, we can leverage NAT's configuration system to easily switch between different LLMs, adjust parameters, or even compose multiple agents together, all through YAML configuration.\n",
    "\n",
    "For example, you can easily test different models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config_8b.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config_8b.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta-llama/llama-3.1-8b-instruct\n",
    "    base_url: http://0.0.0.0:8000/v1\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:34 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config_8b.yml'\n",
      "2025-12-04 11:05:34 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:40 - Initializing LangGraph agent with NAT-managed LLM: nim_llm\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 1\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:34 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:05:37 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The current World Cup holder is the Argentina national team. They defeated the French national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:05:37 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team. They defeated the French national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config_8b.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also serve your LangGraph agent as an API endpoint:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting: Improving Response Quality\n",
    "\n",
    "### Problem: Vague or Incomplete Responses\n",
    "\n",
    "If you see responses like *\"The winner of the last World Cup was not specified in the search results\"* when the information is available, here's why and how to fix it:\n",
    "\n",
    "#### Common Causes:\n",
    "\n",
    "1. **Too Few Search Results** (max_results=2)\n",
    "   - Only 2 results may miss key information\n",
    "   - **Solution**: Increase to 5-10 results\n",
    "\n",
    "2. **Temperature Too Low** (temperature=0.0)\n",
    "   - Makes the model overly conservative\n",
    "   - **Solution**: Use 0.2-0.3 for better reasoning\n",
    "\n",
    "3. **Token Limits** (max_tokens=1024)\n",
    "   - May cut off the agent's reasoning\n",
    "   - **Solution**: Increase to 2048+\n",
    "\n",
    "4. **Search Tool Limitations**\n",
    "   - Tavily may not always return the best results\n",
    "   - **Solution**: Try different queries or add multiple search tools\n",
    "\n",
    "#### Quick Fix Example:\n",
    "\n",
    "```python\n",
    "# Instead of:\n",
    "search = TavilySearch(max_results=2)\n",
    "llm = ChatNVIDIA(model=\"...\", temperature=0.0, max_tokens=1024)\n",
    "\n",
    "# Use:\n",
    "search = TavilySearch(max_results=5)\n",
    "llm = ChatNVIDIA(model=\"...\", temperature=0.2, max_tokens=2048)\n",
    "```\n",
    "\n",
    "#### In YAML Config:\n",
    "\n",
    "```yaml\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  model_name: meta/llama-3.3-70b-instruct\n",
    "  temperature: 0.2        # Higher for better reasoning\n",
    "  max_tokens: 2048        # More room for complete answers\n",
    "  max_search_results: 5   # More context\n",
    "  verbose: true           # See what's happening\n",
    "```\n",
    "\n",
    "#### Alternative: Add a Custom System Prompt\n",
    "\n",
    "```python\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "system_message = \"\"\"You are a helpful assistant with access to search tools.\n",
    "When you find information in search results, provide direct, complete answers.\n",
    "Always cite the source of your information.\"\"\"\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    messages_modifier=system_message\n",
    ")\n",
    "```\n",
    "\n",
    "This is particularly important for LangGraph agents compared to LangChain's AgentExecutor, as the default system prompts may differ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to serve the agent (this will run in the background)\n",
    "# !nat serve --config_file langgraph_agent_workflow/configs/config.yml --host 0.0.0.0 --port 8000\n",
    "# Then visit http://localhost:8000/docs for the API documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next-steps\"></a>\n",
    "# 3) Next Steps\n",
    "\n",
    "Congratulations! You've successfully integrated a LangGraph agent with the NeMo Agent Toolkit. Here are some next steps to explore:\n",
    "\n",
    "## Advanced LangGraph Features\n",
    "\n",
    "1. **Add Custom Tools**: Extend your agent with custom tools beyond web search\n",
    "   ```python\n",
    "   from langchain.tools import tool\n",
    "   \n",
    "   @tool\n",
    "   def custom_calculator(expression: str) -> str:\n",
    "       \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "       return str(eval(expression))\n",
    "   \n",
    "   tools = [search, custom_calculator]\n",
    "   ```\n",
    "\n",
    "2. **Build Custom Graphs**: Create specialized workflows with custom state\n",
    "   ```python\n",
    "   from langgraph.graph import StateGraph, START, END\n",
    "   from typing import TypedDict, Annotated\n",
    "   from langgraph.graph.message import add_messages\n",
    "   \n",
    "   class AgentState(TypedDict):\n",
    "       messages: Annotated[list, add_messages]\n",
    "       context: str\n",
    "   \n",
    "   graph = StateGraph(AgentState)\n",
    "   graph.add_node(\"planner\", planner_node)\n",
    "   graph.add_node(\"executor\", executor_node)\n",
    "   # Add edges and compile\n",
    "   ```\n",
    "\n",
    "3. **Multi-Agent Systems**: Compose multiple LangGraph agents together\n",
    "4. **Human-in-the-Loop**: Add approval steps in your graph\n",
    "5. **Conditional Routing**: Use conditional edges for complex logic\n",
    "\n",
    "## NAT Integration Features\n",
    "\n",
    "6. **Observability**: Add tracing and monitoring\n",
    "   ```yaml\n",
    "   tracing:\n",
    "     _type: phoenix\n",
    "     endpoint: http://localhost:6006\n",
    "   \n",
    "   workflow:\n",
    "     _type: langgraph_agent_workflow\n",
    "     llm_name: nim_llm\n",
    "     tracing_name: phoenix\n",
    "   ```\n",
    "\n",
    "7. **Memory Integration**: Add persistent memory\n",
    "   ```yaml\n",
    "   memory:\n",
    "     _type: redis\n",
    "     host: localhost\n",
    "     port: 6379\n",
    "   \n",
    "   workflow:\n",
    "     _type: langgraph_agent_workflow\n",
    "     llm_name: nim_llm\n",
    "     memory_name: redis\n",
    "   ```\n",
    "\n",
    "8. **Evaluation**: Use NAT's evaluation tools\n",
    "   ```bash\n",
    "   nat eval --config_file config.yml --dataset eval_dataset.json\n",
    "   ```\n",
    "\n",
    "## Key Differences: LangGraph vs LangChain Agents\n",
    "\n",
    "| Feature | LangChain Agent | LangGraph Agent |\n",
    "|---------|----------------|-----------------|\n",
    "| **Architecture** | AgentExecutor | StateGraph |\n",
    "| **State Management** | Dict-based | Message-based with custom state |\n",
    "| **Input Format** | `{\"input\": ..., \"chat_history\": []}` | `{\"messages\": [(\"user\", ...)]}` |\n",
    "| **Output Format** | `response[\"output\"]` | `response[\"messages\"][-1].content` |\n",
    "| **Flexibility** | Limited | High (custom nodes/edges) |\n",
    "| **Multi-Agent** | Difficult | Native support |\n",
    "| **Human-in-Loop** | Manual implementation | Built-in support |\n",
    "| **Conditional Logic** | Limited | Full control with conditional edges |\n",
    "| **Production Use** | Good for simple cases | Better for complex workflows |\n",
    "\n",
    "## When to Use LangGraph\n",
    "\n",
    "- **Complex agent workflows** with conditional logic\n",
    "- **Multi-agent collaboration** scenarios\n",
    "- When you need **fine-grained control** over execution\n",
    "- **Human-in-the-loop** workflows\n",
    "- Building **production-grade agentic applications**\n",
    "- **State management** across multiple steps\n",
    "- **Parallel execution** of independent tasks\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\n",
    "- [NeMo Agent Toolkit Documentation](https://docs.nvidia.com/nemo-agent-toolkit/)\n",
    "- [NVIDIA NIM](https://docs.nvidia.com/nim/)\n",
    "- [LangGraph Examples](https://github.com/langchain-ai/langgraph/tree/main/examples)\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated three progressive levels of LangGraph integration with NAT:\n",
    "\n",
    "1. **V1 (Basic)**: Quick wrap of existing agent with minimal changes\n",
    "2. **V2 (Configurable)**: Parameters exposed in YAML for easy experimentation\n",
    "3. **V3 (Full Integration)**: NAT-managed components for production use\n",
    "\n",
    "Choose the version that best fits your needs, and progressively enhance your integration as your requirements grow!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Adding Observability and Profiling\n",
    "\n",
    "Now that you have a working LangGraph agent, let's add observability and profiling to understand how your agent is performing and where time is being spent.\n",
    "\n",
    "## 4.1) Observability with Phoenix\n",
    "\n",
    "Phoenix provides real-time tracing and visualization of your agent's execution, showing each step, token usage, and latency.\n",
    "\n",
    "### Step 1: Install Phoenix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Phoenix installation complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix version: 12.19.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install Phoenix server and NAT integration\n",
    "uv pip show -q \"arize-phoenix\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Installing Phoenix server...\"\n",
    "    uv pip install arize-phoenix\n",
    "fi\n",
    "\n",
    "uv pip show -q \"nvidia-nat-phoenix\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Installing NAT Phoenix integration...\"\n",
    "    uv pip install \"nvidia-nat[phoenix]\"\n",
    "fi\n",
    "\n",
    "echo \"‚úÖ Phoenix installation complete!\"\n",
    "echo \"Phoenix version: $(python -c 'import phoenix; print(phoenix.__version__)' 2>/dev/null || echo 'installed')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start Phoenix Server\n",
    "\n",
    "Phoenix runs as a local server that collects and visualizes traces from your agent.\n",
    "\n",
    "**Important**: Phoenix must be started in a **separate terminal** before running your agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PHOENIX_HOST=0.0.0.0\n"
     ]
    }
   ],
   "source": [
    "%env PHOENIX_HOST=0.0.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will start the Phoenix server in the background:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Phoenix will run on port 6006\n",
    "phoenix serve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important: Don't Run `!phoenix serve` in Notebook\n",
    "\n",
    "**Why not:**\n",
    "- It's a long-running server that will block the notebook cell\n",
    "- You won't be able to run other cells\n",
    "- The server stops when you interrupt the cell\n",
    "\n",
    "**Instead:** Use a separate terminal (see instructions above) or use the background method below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phoenix...\n",
      "   Waiting... (1/10)\n",
      "   Waiting... (2/10)\n",
      "   Waiting... (3/10)\n",
      "‚úÖ Phoenix started successfully!\n",
      "üåê Access UI at: http://localhost:6006\n",
      "üìä Process ID: 306342\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Start Phoenix in the background\n",
    "try:\n",
    "    # Kill any existing Phoenix processes\n",
    "    subprocess.run([\"pkill\", \"-f\", \"phoenix\"], stderr=subprocess.DEVNULL)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Start Phoenix in background\n",
    "    phoenix_process = subprocess.Popen(\n",
    "        [\"phoenix\", \"serve\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        cwd=\"/home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks\"\n",
    "    )\n",
    "    \n",
    "    # Wait for Phoenix to start\n",
    "    print(\"Starting Phoenix...\")\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:6006\", timeout=1)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úÖ Phoenix started successfully!\")\n",
    "                print(\"üåê Access UI at: http://localhost:6006\")\n",
    "                print(f\"üìä Process ID: {phoenix_process.pid}\")\n",
    "                break\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            print(f\"   Waiting... ({i+1}/10)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Phoenix may not have started. Check manually.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'phoenix' command not found.\")\n",
    "    print(\"\\nPlease use a separate terminal instead:\")\n",
    "    print(\"  cd /home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks\")\n",
    "    print(\"  python -m phoenix.server.main serve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stop Phoenix later:**\n",
    "```python\n",
    "import subprocess\n",
    "subprocess.run([\"pkill\", \"-f\", \"phoenix\"])\n",
    "print(\"Phoenix stopped\")\n",
    "```\n",
    "\n",
    "**To check if Phoenix is running:**\n",
    "```python\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:6006\", timeout=1)\n",
    "    print(f\"‚úÖ Phoenix is running (status: {response.status_code})\")\n",
    "except:\n",
    "    print(\"‚ùå Phoenix is not running\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Simplified Approach (Most Reliable)\n",
    "\n",
    "**Step-by-step Phoenix setup:**\n",
    "\n",
    "1. **Open a NEW terminal window**\n",
    "\n",
    "2. **Navigate to the notebook directory:**\n",
    "   ```bash\n",
    "   cd /home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks\n",
    "   ```\n",
    "\n",
    "3. **Start Phoenix:**\n",
    "   ```bash\n",
    "   phoenix serve\n",
    "   ```\n",
    "   \n",
    "   If that doesn't work, try:\n",
    "   ```bash\n",
    "   python -m phoenix.server.main serve\n",
    "   ```\n",
    "\n",
    "4. **You should see:**\n",
    "   ```\n",
    "   Phoenix server running on http://127.0.0.1:6006\n",
    "   ```\n",
    "\n",
    "5. **Open in browser:** http://localhost:6006\n",
    "\n",
    "6. **Keep that terminal open** - closing it stops Phoenix\n",
    "\n",
    "**That's it!** Phoenix is now ready to collect traces. Proceed to the next cell to configure your agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Update Config with Tracing\n",
    "\n",
    "Now we'll update the workflow configuration to enable Phoenix tracing. We'll append the telemetry section to the existing config:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp langgraph_agent_workflow/configs/config.yml langgraph_agent_workflow/configs/config_with_tracing.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to langgraph_agent_workflow/configs/config_with_tracing.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a langgraph_agent_workflow/configs/config_with_tracing.yml\n",
    "\n",
    "general:\n",
    "  telemetry:\n",
    "    logging:\n",
    "      console:\n",
    "        _type: console\n",
    "        level: WARN\n",
    "    tracing:\n",
    "      phoenix:\n",
    "        _type: phoenix\n",
    "        endpoint: http://localhost:6006/v1/traces\n",
    "        project: langgraph_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Agent with Tracing\n",
    "\n",
    "Now run your agent and traces will be automatically sent to Phoenix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:46 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config_with_tracing.yml'\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 1\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "--------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config_with_tracing.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: View Traces in Phoenix UI\n",
    "\n",
    "After running the agent, open the Phoenix UI in your browser to see:\n",
    "\n",
    "1. **Visit**: http://localhost:6006\n",
    "2. **View**:\n",
    "   - Complete trace of agent execution\n",
    "   - Each LLM call with prompts and responses\n",
    "   - Tool calls and their results\n",
    "   - Token usage per step\n",
    "   - Latency breakdown\n",
    "   - Full execution timeline\n",
    "\n",
    "**What You'll See:**\n",
    "- üîç **Spans**: Each operation (LLM call, tool call) as a span\n",
    "- ‚è±Ô∏è **Timing**: How long each step took\n",
    "- üìä **Token Usage**: Input/output tokens per LLM call\n",
    "- üîó **Flow**: Visual graph of agent execution\n",
    "- üìù **Prompts & Responses**: Full text of all interactions\n",
    "\n",
    "This is incredibly valuable for:\n",
    "- Debugging agent behavior\n",
    "- Optimizing performance\n",
    "- Understanding token usage\n",
    "- Identifying bottlenecks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Evaluation Dataset\n",
    "\n",
    "Profiling in NAT works through the `nat eval` command. First, create a simple evaluation dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/data/eval_data.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/data/eval_data.json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"question\": \"Who won the last World Cup?\",\n",
    "        \"answer\": \"Argentina won the 2022 FIFA World Cup. They defeated France in the final with a score of 3-3 (4-2 on penalties) in Qatar on December 18, 2022.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"question\": \"What year did the last World Cup take place?\",\n",
    "        \"answer\": \"The last FIFA World Cup took place in 2022. It was held in Qatar from November 21 to December 18, 2022.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"question\": \"Which country hosted the 2022 World Cup?\",\n",
    "        \"answer\": \"Qatar hosted the 2022 FIFA World Cup. It was the first World Cup held in the Middle East and the first held in November-December rather than the traditional June-July timeframe.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Update Config with Profiler Settings\n",
    "\n",
    "Add profiler configuration to enable detailed performance analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Profiling with NAT's Built-in Profiler\n",
    "\n",
    "NAT includes built-in profiling that measures CPU time, memory usage, and execution time for each component.\n",
    "\n",
    "### Enable Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /home/ubuntu/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install profiling dependencies\n",
    "uv pip show -q \"memray\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    uv pip install \"nvidia-nat[profiling]\"\n",
    "else\n",
    "    echo \"Profiling tools are already installed\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config_with_profiling.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config_with_profiling.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta-llama/llama-3.1-8b-instruct\n",
    "    base_url: http://0.0.0.0:8000/v1\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "general:\n",
    "  telemetry:\n",
    "    logging:\n",
    "      console:\n",
    "        _type: console\n",
    "        level: INFO\n",
    "    tracing:\n",
    "      phoenix:\n",
    "        _type: phoenix\n",
    "        endpoint: http://localhost:6006/v1/traces\n",
    "        project: langgraph_agent\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./profile_output\n",
    "    verbose: true\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: ./langgraph_agent_workflow/data/eval_data.json\n",
    "    \n",
    "    profiler:\n",
    "      token_uniqueness_forecast: true\n",
    "      workflow_runtime_forecast: true\n",
    "      compute_llm_metrics: true\n",
    "      csv_exclude_io_text: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Config with Profiling\n",
    "\n",
    "Create a complete config file that includes workflow, tracing, and evaluation with profiling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Troubleshooting: Python Environment Issues\n",
    "\n",
    "### Problem: \"No module named pip\"\n",
    "\n",
    "This error occurs when pip is not installed in your virtual environment.\n",
    "\n",
    "**Quick Fix (Run in notebook):**\n",
    "\n",
    "```python\n",
    "import sys\n",
    "!{sys.executable} -m ensurepip --default-pip\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "```\n",
    "\n",
    "**Permanent Fix (Run in terminal):**\n",
    "\n",
    "```bash\n",
    "# 1. Navigate to your project directory\n",
    "cd /path/to/your/project\n",
    "\n",
    "# 2. Deactivate current venv if active\n",
    "deactivate\n",
    "\n",
    "# 3. Remove old venv\n",
    "rm -rf .venv\n",
    "\n",
    "# 4. Create new venv with system packages\n",
    "python3 -m venv .venv --system-site-packages\n",
    "\n",
    "# 5. Activate it\n",
    "source .venv/bin/activate  # macOS/Linux\n",
    "# OR\n",
    ".venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# 6. Ensure pip is installed\n",
    "python -m ensurepip --upgrade\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "# 7. Install Jupyter in the venv\n",
    "pip install jupyter ipykernel\n",
    "\n",
    "# 8. Register the kernel\n",
    "python -m ipykernel install --user --name=nat-env --display-name=\"Python (NAT)\"\n",
    "\n",
    "# 9. Restart Jupyter and select the \"Python (NAT)\" kernel\n",
    "```\n",
    "\n",
    "**Verify Installation:**\n",
    "\n",
    "```python\n",
    "import sys\n",
    "import pip\n",
    "print(f\"Python: {sys.executable}\")\n",
    "print(f\"pip version: {pip.__version__}\")\n",
    "```\n",
    "\n",
    "### Problem: Wrong Python Environment\n",
    "\n",
    "**Symptoms:**\n",
    "- Packages installed but not found\n",
    "- Import errors after installation\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "1. **Check which Python is being used:**\n",
    "   ```python\n",
    "   import sys\n",
    "   print(sys.executable)\n",
    "   ```\n",
    "\n",
    "2. **In Jupyter, always use:**\n",
    "   ```python\n",
    "   import sys\n",
    "   !{sys.executable} -m pip install package_name\n",
    "   ```\n",
    "   Instead of:\n",
    "   ```python\n",
    "   %pip install package_name\n",
    "   # or\n",
    "   !pip install package_name\n",
    "   ```\n",
    "\n",
    "### Problem: Virtual Environment Not Activated\n",
    "\n",
    "**Check if in venv:**\n",
    "```python\n",
    "import os\n",
    "print(os.getenv('VIRTUAL_ENV', 'Not in a virtual environment'))\n",
    "```\n",
    "\n",
    "**Activate venv in terminal:**\n",
    "```bash\n",
    "source /home/ubuntu/.venv/bin/activate  # Your path\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Troubleshooting Local NIM Issues\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### 1. Connection Refused / Cannot Connect\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ConnectionError: Cannot connect to local NIM at http://0.0.0.0:8000\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "- Verify NIM is running: `docker ps | grep nim` or check your NIM process\n",
    "- Try using `localhost` instead of `0.0.0.0`:\n",
    "  ```yaml\n",
    "  base_url: http://localhost:8000/v1\n",
    "  ```\n",
    "- Check firewall settings\n",
    "- Verify the port (8000) is correct for your NIM deployment\n",
    "\n",
    "#### 2. Model Not Found\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "Error: model 'meta-llama/llama-3.1-8b-instruct' not found\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "- List available models:\n",
    "  ```bash\n",
    "  curl http://0.0.0.0:8000/v1/models\n",
    "  ```\n",
    "- Update `model_name` in config to match an available model\n",
    "- Ensure your NIM container has the model downloaded\n",
    "\n",
    "#### 3. Slow Response Times\n",
    "\n",
    "**Symptoms:**\n",
    "- Requests taking longer than expected\n",
    "- Timeouts\n",
    "\n",
    "**Solutions:**\n",
    "- Check GPU utilization: `nvidia-smi`\n",
    "- Reduce `max_tokens` in config\n",
    "- Ensure NIM has enough GPU memory\n",
    "- Check if other processes are using the GPU\n",
    "\n",
    "#### 4. Authentication Errors\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "401 Unauthorized\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "- If your NIM requires auth, set proper API key:\n",
    "  ```python\n",
    "  os.environ[\"NVIDIA_API_KEY\"] = \"your-local-nim-key\"\n",
    "  ```\n",
    "- Or configure it in the LLM config:\n",
    "  ```yaml\n",
    "  llms:\n",
    "    nim_llm:\n",
    "      api_key: your-local-key\n",
    "  ```\n",
    "\n",
    "#### 5. Testing NIM Directly (Bypass NAT)\n",
    "\n",
    "If workflows aren't working, test the NIM directly:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    'http://0.0.0.0:8000/v1/chat/completions',\n",
    "    json={\n",
    "        \"model\": \"meta-llama/llama-3.1-8b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    ")\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "### Performance Tuning for Local NIM\n",
    "\n",
    "**For Better Throughput:**\n",
    "```yaml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    model_name: meta-llama/llama-3.1-8b-instruct\n",
    "    base_url: http://0.0.0.0:8000/v1\n",
    "    temperature: 0.7\n",
    "    max_tokens: 1024  # Reduce for faster responses\n",
    "    top_p: 0.9\n",
    "```\n",
    "\n",
    "**For Better Quality (Slower):**\n",
    "```yaml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    model_name: meta-llama/llama-3.1-8b-instruct\n",
    "    base_url: http://0.0.0.0:8000/v1\n",
    "    temperature: 0.2  # More focused\n",
    "    max_tokens: 4096  # Longer responses\n",
    "    top_p: 0.95\n",
    "```\n",
    "\n",
    "### Monitoring Your Local NIM\n",
    "\n",
    "**Check NIM Logs:**\n",
    "```bash\n",
    "# If running in Docker\n",
    "docker logs <nim-container-id>\n",
    "\n",
    "# Check GPU usage\n",
    "watch -n 1 nvidia-smi\n",
    "```\n",
    "\n",
    "**Monitor Performance:**\n",
    "- Use the profiling features in this notebook (Cell 71+)\n",
    "- Enable verbose logging in configs\n",
    "- Use Phoenix tracing to see request/response times\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation with Profiling\n",
    "\n",
    "Run the evaluation which will automatically generate profiling data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:06:08 - INFO     - nat.eval.evaluate:446 - Starting evaluation run with config file: langgraph_agent_workflow/configs/config_with_profiling.yml\n",
      "2025-12-04 11:06:08 - INFO     - phoenix.config:1750 - üìã Ensuring phoenix working directory: /home/ubuntu/.phoenix\n",
      "2025-12-04 11:06:08 - INFO     - phoenix.inferences.inferences:112 - Dataset: phoenix_inferences_3e86ea26-8d55-4f3e-8539-cc3842854e18 initialized\n",
      "2025-12-04 11:06:10 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:40 - Initializing LangGraph agent with NAT-managed LLM: nim_llm\n",
      "Running workflow:   0%|                                   | 0/3 [00:00<?, ?it/s]2025-12-04 11:06:10 - INFO     - nat.observability.exporter_manager:269 - Started exporter 'phoenix'\n",
      "2025-12-04 11:06:10 - INFO     - nat.observability.exporter_manager:269 - Started exporter 'phoenix'\n",
      "2025-12-04 11:06:10 - INFO     - nat.observability.exporter_manager:269 - Started exporter 'phoenix'\n",
      "2025-12-04 11:06:10 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:06:27 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:06:27 - INFO     - nat.observability.exporter.base_exporter:283 - Event stream completed. No more events will arrive.\n",
      "2025-12-04 11:06:27 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: What year did the last World Cup take place?\n",
      "2025-12-04 11:06:44 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The last World Cup took place in 2022.\n",
      "2025-12-04 11:06:44 - INFO     - nat.observability.exporter.base_exporter:283 - Event stream completed. No more events will arrive.\n",
      "2025-12-04 11:06:44 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Which country hosted the 2022 World Cup?\n",
      "2025-12-04 11:07:01 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The country that hosted the 2022 World Cup was Qatar.\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter.base_exporter:283 - Event stream completed. No more events will arrive.\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter_manager:275 - Stopped exporter 'phoenix'\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter_manager:275 - Stopped exporter 'phoenix'\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter_manager:275 - Stopped exporter 'phoenix'\n",
      "Running workflow: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:51<00:00, 17.06s/it]\n",
      "2025-12-04 11:07:01 - WARNING  - nat.eval.evaluate:377 - All evaluators were empty or invalid.\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:426 - Waiting for export tasks from 1 local exporters (timeout: 60s)\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:431 - Export tasks completed for exporter: phoenix\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:435 - All local export task waiting completed\n",
      "2025-12-04 11:07:01 - INFO     - nat.profiler.profile_runner:127 - Wrote combined data to: profile_output/all_requests_profiler_traces.json\n",
      "2025-12-04 11:07:01 - INFO     - nat.profiler.profile_runner:146 - Wrote merged standardized DataFrame to profile_output/standardized_data_all.csv\n",
      "2025-12-04 11:07:01 - INFO     - nat.profiler.profile_runner:200 - Wrote inference optimization results to: profile_output/inference_optimization.json\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:335 - Workflow output written to profile_output/workflow_output.json\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mkdir -p langgraph_agent_workflow/data\n",
    "!nat eval --config_file langgraph_agent_workflow/configs/config_with_profiling.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: View Profiling Results\n",
    "\n",
    "After the evaluation completes, check the profiling output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Profiling Output Files ===\n",
      "total 296K\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 212K Dec  4 11:07 all_requests_profiler_traces.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1.7K Dec  4 11:07 inference_optimization.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 6.3K Dec  4 11:07 standardized_data_all.csv\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  70K Dec  4 11:07 workflow_output.json\n",
      "\n",
      "Expected files:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - all_requests_profiler_traces.json  (Raw LLM traces)\n",
      "  - inference_optimization.json        (Performance metrics)\n",
      "  - standardized_data_all.csv          (Token usage data)\n"
     ]
    }
   ],
   "source": [
    "!echo \"=== Profiling Output Files ===\"\n",
    "!ls -lh ./profile_output/ 2>/dev/null || echo \"Run the evaluation cell above first\"\n",
    "!echo \"\"\n",
    "!echo \"Expected files:\"\n",
    "!echo \"  - all_requests_profiler_traces.json  (Raw LLM traces)\"\n",
    "!echo \"  - inference_optimization.json        (Performance metrics)\"\n",
    "!echo \"  - standardized_data_all.csv          (Token usage data)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "After running evaluation with profiling, you'll find several files in `./profile_output/`:\n",
    "\n",
    "#### Core Output Files\n",
    "\n",
    "**1. `all_requests_profiler_traces.json`**\n",
    "- Raw traces of all LLM interactions\n",
    "- Tool input and output data\n",
    "- Runtime measurements for each component\n",
    "- Complete execution metadata\n",
    "\n",
    "**2. `inference_optimization.json`**\n",
    "- Workflow performance metrics with confidence intervals\n",
    "- 90%, 95%, and 99% confidence intervals for latency\n",
    "- Throughput statistics\n",
    "- Workflow runtime predictions\n",
    "- Token usage forecasts\n",
    "\n",
    "**3. `standardized_data_all.csv`**\n",
    "- Standardized usage data in CSV format\n",
    "- Prompt tokens and completion tokens per request\n",
    "- LLM input/output text\n",
    "- Framework information (LangGraph)\n",
    "- Timing and metadata for each evaluation question\n",
    "\n",
    "#### Advanced Analysis Files (if enabled in config)\n",
    "\n",
    "**4. Analysis Reports**\n",
    "- **Bottleneck analysis**: Identifies slowest components in your workflow\n",
    "- **Concurrency analysis**: Shows parallel execution opportunities\n",
    "- **Token uniqueness forecast**: Predicts token efficiency for future queries\n",
    "\n",
    "### Key Metrics to Watch\n",
    "\n",
    "| Metric | Description | Where to Find |\n",
    "|--------|-------------|---------------|\n",
    "| **Total Latency** | End-to-end response time | `inference_optimization.json` |\n",
    "| **Token Usage** | Input/output tokens per request | `standardized_data_all.csv` |\n",
    "| **LLM Time** | Time spent in LLM calls | `all_requests_profiler_traces.json` |\n",
    "| **Tool Time** | Time spent in search/tools | Trace JSON, individual tool spans |\n",
    "| **Cost Estimate** | Approximate API costs | Calculate from token counts |\n",
    "\n",
    "### Example: Viewing Results\n",
    "\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# View optimization metrics\n",
    "with open('./profile_output/inference_optimization.json') as f:\n",
    "    metrics = json.load(f)\n",
    "    print(f\"Average latency: {metrics.get('avg_latency', 'N/A')}s\")\n",
    "\n",
    "# View detailed usage data\n",
    "df = pd.read_csv('./profile_output/standardized_data_all.csv')\n",
    "print(f\"Total tokens used: {df['prompt_tokens'].sum() + df['completion_tokens'].sum()}\")\n",
    "print(f\"Average response time: {df['execution_time'].mean():.2f}s\")\n",
    "```\n",
    "\n",
    "### Optimization Tips\n",
    "\n",
    "1. **High LLM Time**: \n",
    "   - Use smaller models (8B instead of 70B)\n",
    "   - Reduce max_tokens\n",
    "   - Cache common queries\n",
    "\n",
    "2. **High Tool Time**:\n",
    "   - Reduce max_search_results\n",
    "   - Use faster search APIs\n",
    "   - Implement tool result caching\n",
    "\n",
    "3. **High Memory**:\n",
    "   - Reduce conversation history\n",
    "   - Clear unused variables\n",
    "   - Use streaming responses\n",
    "\n",
    "4. **High Token Usage**:\n",
    "   - Optimize prompts\n",
    "   - Reduce search result content\n",
    "   - Use more focused tool descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Combined: Observability + Profiling Best Practices\n",
    "\n",
    "### Recommended Development Workflow\n",
    "\n",
    "1. **Development** (Local):\n",
    "   - Enable verbose logging\n",
    "   - Use Phoenix for tracing\n",
    "   - Profile periodically\n",
    "\n",
    "2. **Testing** (Pre-Production):\n",
    "   - Enable profiling for all test runs\n",
    "   - Monitor memory usage\n",
    "   - Track token consumption\n",
    "\n",
    "3. **Production**:\n",
    "   - Lightweight tracing (sample rate)\n",
    "   - Continuous performance monitoring\n",
    "   - Alert on anomalies\n",
    "\n",
    "### Example Combined Configuration\n",
    "\n",
    "```yaml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.3-70b-instruct\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "tracing:\n",
    "  phoenix_tracer:\n",
    "    _type: phoenix\n",
    "    endpoint: http://localhost:6006\n",
    "    sample_rate: 1.0  # 100% in dev, lower in prod\n",
    "\n",
    "profiling:\n",
    "  enabled: true\n",
    "  output_dir: ./profiling_results\n",
    "  profile_memory: true\n",
    "  profile_cpu: true\n",
    "\n",
    "logging:\n",
    "  level: INFO\n",
    "  format: json\n",
    "  output: ./logs/agent.log\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true\n",
    "  tracing_name: phoenix_tracer\n",
    "```\n",
    "\n",
    "### Quick Debugging Commands\n",
    "\n",
    "```bash\n",
    "# View traces in real-time\n",
    "open http://localhost:6006\n",
    "\n",
    "# Check profiling results\n",
    "ls -lh ./profiling_results/\n",
    "\n",
    "# View memory profile\n",
    "memray flamegraph ./profiling_results/memory.bin\n",
    "\n",
    "# Analyze CPU profile\n",
    "python -m pstats ./profiling_results/cpu_profile.prof\n",
    "\n",
    "# Check logs\n",
    "tail -f ./logs/agent.log | jq '.'\n",
    "```\n",
    "\n",
    "### Benefits of Observability + Profiling\n",
    "\n",
    "‚úÖ **Faster Debugging**: See exactly what the agent is doing  \n",
    "‚úÖ **Performance Optimization**: Identify bottlenecks quickly  \n",
    "‚úÖ **Cost Management**: Track token usage and API calls  \n",
    "‚úÖ **Quality Assurance**: Verify agent behavior  \n",
    "‚úÖ **Production Readiness**: Monitor health in real-time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
